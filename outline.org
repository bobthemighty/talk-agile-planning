#+title: Outline
* Why do we need to plan?
The first question is why do we need to plan? this might sound like a stupid question to anyone who isn't a software engineer, but most software engineers think of planning as an intrusion on their time, and an affront to their professionalism. The general attitude among software people is "it will be done when it's done. Talking about it won't make it happen any faster, so can I just go and get on with it?"

In fact, I think there are three valid reasons for planning our work.
** Hard external commitments
The first is the worst case. Hard external commitments. This is where we have some date to work toward that we don't have control over. For example, when we launched Cazoo in Europe, we'd already signed deals to sponsor a bunch of football teams. We needed to have Cazoo up and running in Europe before the start of the football season. When we launched in the UK, we had millions of pounds worth of TV and radio advertising booked. There is a long lead time for TV advertising and you can't change the date at the last minute.
In this situation, planning tells us whether or not we are likely to hit the target date. We need to know this information so that we can take corrective action if we're going to miss it. Corrective action might mean getting some more help externally, though that often doesn't help short term, or cutting scope to be more realistic about what we can deliver in the remaining time.
** Pipeline management
The second reason is pipeline management. When salespeople go out to talk to cement manufacturers, or when our customers have a cunning plan for some new feature that would help operators, they need to understand when we might be able to deliver work. The risk with sales people is that they sell things you can't ship. The anti-pattern is that dates materialise spontaneously - we've signed a deal with X, and we need to start a trial by Y. The problem with this model is that it's disruptive, because we need to drop other commitments, or shelve existing work, in order to hit a target date that we've imposed on ourselves. What we need is a clear roadmap so we can see when there is capacity available to deliver new work.
** Continuous improvement
The third reason to plan and track work is so that we can improve our delivery. You can't improve what you don't measure. In order for us to get better at shipping software, more quickly, more predictably, we'll need to see what we intended to deliver, and what we actually delivered, and why. That gives us the data to try different ways of working, or to fix problems in our process that hinder our progress.
** There is no such thing as a deadline
It's important to note that there is almost never such a thing as a deadline in programming. A deadline is a date that you have to hit, or the work is _wasted_. If you're a journalist, you have a deadline. If your story isn't ready to go to press on time, it won't go at all. For programmers, that's almost never the case. We'd always ship late rather than not ship at all. If you're not going to cancel the work when you hit the deadline, then it isn't really a deadline, it's an _aspiration_, or a prediction about our performance.
* Software planning is hard
Software planning is tricky because it's hard to know how long a single piece of work will take, and it's hard to figure out how many pieces of work are required for a given project.

** Novelty
Some of this is because pretty much everything we do in software is _new_. It's unusual in a product focused company to be churning out the same work day after day. Instead, every piece of work we do requires us to solve new problems, and it's difficult to predict how long a problem will take to solve. When we first see a project, we might break it down into 5 or 6 pieces of work, but when we come to those pieces, we'll find that there are new complexities, or things we forgot about, that make the work much more involved than expected. This is the nature of software. Programming is a _design_ process, not a manufacturing process.
** Complexity
Software is complex. I don't mean that it's hard, I mean that the behaviour of the system emerges from the interactions of the parts. As we add more parts to the system, we introduce unexpected interactions - bugs - that slow us down. The other problem is that when we describe a piece of software, we use informal, human language to explain ourselves, and then have to translate that description into a formal structured form - code. This translation is sometimes imperfect where people make assumptions, or misread ambiguity.
** Yak Shaving
Some of the difficulty comes from yak-shaving. This is a programmer term for "work you have to do before you can do the work you want to do". Common sources of yak-shaving include manual workarounds for long-standing problems, difficulties deploying code to a test environment, issues with upgrading third party components, and so on. This is toil, it's unproductive work that doesn't directly deliver value. For example, the other day I wanted to build a demo for a talk I was doing. The code itself took me about 25 minutes to write. Creating a working demonstration, after battling javascript bundlers and language quirks, took me another 90 minutes on top. That's a lot of toil. The worst thing about the toil is that it's often unforeseeable. I had no way of knowing that I'd waste an hour and a half on trivia before my code would run the way I wanted.
** Change
The great thing about software is that it's soft - we can change it. The bad thing about software is that people want to change it all the time. A lot of this is down to novelty. It's hard to foresee, at the beginning of a project, how the thing will actually work, or how users will want to engage with your software. As a result, requirements tend to change - often dramatically - as a project proceeds.
* The dark ages: waterfall
I want to talk about some ways we've tried to plan software in the past, so that you're familiar with the terminology. Some of you might know this stuff already, in which case I'm sorry to bore you.
** Phased project delivery
Back in the early days of software engineering, we tried to apply the processes that we'd used in other industries. The "waterfall" model of software engineering is a phased process where each phase depends on the outputs from the previous phase.
First, you work out the requirements, then you create a design for software that meets those requirements, and you get your customer to "sign off" that document. Next you break the work into tasks and divvy them up among your programmers and estimate how long they'll each take. When they've done their work, you plug it all together and test it, then get the customer to sign off that it works. Lastly, you deploy the whole thing to production and enter a maintenance phase where you fix bugs and keep it all humming.
** This sucks
We've already explained why this doesn't work.
*** Novelty
Many of the tasks you come up with will turn out to be harder than you expected, or to contain other tasks for which you didn't account. It's common to just inflate the size of the project by a third to a half to cover unexpected difficulties.
*** Complexity
It was common, in the dark ages, to have an "integration phase" where you take the work from all your programmers, who were building separate modules of the software, and plug them together. Inevitably, when you do this, you realise that ambiguities and assumptions mean the behaviour of the whole isn't as you expected, even though the behaviour of the parts make sense individually. Fixing these cognitive errors can take as long as writing the thing in the first place.
*** Yak Shaving
The complexity and yak shaving issues mean that it's hard to know ahead of time how long any particular task will take. Using a rigid project-management approach to software engineering inevitably leads to delays. If you've planned to build 20 modules, and some of those modules rely on others, then these delays have a compounding effect. A small delay at the beginning of a project can lead to wild overruns by the end of it. For this reason, historically, 70% of software projects were either cancelled or delivered over time and budget.
*** Change
Complexity and malleability mean that the requirements document you got your customer to sign doesn't mean _anything_. Pragmatically, change is going to happen. Changes to requirements will introduce more complexity, more yak shaving, more delays, and so all of these factors interrelate into painful feedback loops.
** Phased project delivery compounds risk
The underlying theme is risk. Every time we change a piece of software, extending it or deploying it, we incur some risk. Maybe we'll introduce a bug, maybe we'll break the test environment, maybe we'll uncover a gigantic mess that has to be cleaned up before we can move ahead. The sequential model of waterfall compounds this risk. By doing all the development, then the testing, then the deployment we're taking all of our risk and bundling it together into a single mega-risk.
* Agile: the XP years
Extreme Programming was a movement in the 80s and 90s that tried to shake up software delivery and move it away from a project-management focus. XP emphasises communication with the customer, working in small iterative pieces, and continual improvement of quality. The rallying cry for XP was that we should *embrace* change, seeing it as a natural part of the software development process, rather than trying to freeze requirements in time and work to a perfected plan.
** User stories
In XP, the unit of work is a user story. A user story is an index card, where you write at the top something like "as a user, I need to see kg/co2 per kg/clinker so that I can understand how recommendations are affecting my carbon intensivity". I like to use Friends style stories where we just say "The one where the operator checks kg/co2 per kg/clinker".
These items are expressed in terms of user value, not technical tasks. A story might take a day or it might take a week or two. When a user story is delivered, there is some tangible thing that you can show to your customer and get feedback on.
** Story points and velocity
In XP we estimate how hard a story is to deliver, not the time it will take, using story points. Story points are relative units of arbitrary bigness. The way to size them is to take a small or medium story, and say "this story is worth 2 points", and then scale other stories around it. If something is twice as hard, that might be a 5. If something is easier, it might be a 1. Story points capture both the amount of work required (is it a lot, or is this a quick job), and the degree of risk. A small story that has some tricky edge cases might come out as a 3, because we're not confident about the work.

In XP we work in short cycles, typically a couple of weeks, and we count how many story points we delivered each iteration. This is velocity - the number of story points delivered by the team.

Now we can plan future work. We can say to managers, who didn't know anything about software when XP was the new hotness, you've got 20 points available, here are 20 marbles. How do you want to spend them? This curtails stupid conversations where you say "which of these two things do you want?" and they say "I need both, dammit." Now you can say "but you only have two marbles left, and both of these things cost two marbles. How can you have both if you haven't got four marbles?"
** Predicting delivery times
We can also use velocity to predict when things will be finished. If we know that we've got 100 story points left to deliver, and we're averaging 10 points every two weeks, then it's reasonable to assume that we'll be done in 20 weeks. This is a burn-up chart. It's a graph that shows you everything you need to know about the progress of a software project. This thick line at the top shows the _scope_ of the project - how many points remain to deliver. You can see that sometimes it goes up or down when we add things to a project, or cut scope to deliver more quickly. The solid line from the lower left shows our progress through the scope, and this dotted line shows the predicted trajectory.
** Standups
XP introduced the idea of stand up meetings at the beginning of a day. The idea is that we ask ourselves "are we on track to deliver the things we said we'd deliver?" and if not, we take some corrective actions. I've already written a whole thing about standups, so I'm not going to dwell on the topic, except that to note that most standups suck because they turn into status reports, rather than a _planning_ exercise where we decide what's going to get done each day.
** Inspect and adapt
Central to XP is this idea of continuous improvement. Its okay if the very first thing we release is crap, because we're going to make it better over time, releasing a little chunk every couple of weeks so we demonstrate continuous progress to our customers. For that to work we need a culture that encourages us to take stock of what we've done, and how we can do it better.
In XP, we aim to continuously improve our processes through retrospectives.
Retrospectives, like standups, mostly suck. They have a tendency to turn into boring whine fests where nothing ever gets fixed, but the principle that we should continually try to improve our ways of working is solid.
** If it hurts, do it more
I have this other talk I do about XP practices where I propose that the core philosophy of XP is "if it hurts, do it more". Testing is hard, so we test our code on every single change with unit tests. Integrating the work of multiple engineers is hard, so we integrate our code on every change with a continuous integration system. Designing code is hard, so we design as part of every story, refactoring our way to high quality, expressive software.
* Agile: Lean and Kanban
In the early noughties, some of the XP nerds got interested in lean manufacturing processes and, in particular, the Toyota Production system. The fundamental principle in the Toyota Production System is that we want to aim for continuous, predictable, flow.
** Little's Law
To illuminate some of the concepts, it's worth knowing Little's Law. Little's Law is a theorem that describes the behaviour of queuing processes, where work arrives, undergoes some operation, and exits again. Little's Law applies to any such system, from the queues at a supermarket, to the manufacturing processes in a cement plant, to the workings of a software team.
*** L = λW
L is the number of things in the queue. Lambda is the rate that things enter or leave the system, and W is the amount of time that each item spends in the system.

Let's make it concrete. Imagine we have a coffee shop. Customers arrive at some average rate - the _throughput_ of the system, and spend some average amount of time waiting to be served. Let's say that one customer arrives every five minutes. Lambda, then is 1/5, or 0.2. When they arrive, they spend 10 minutes waiting for their coffee, so W is 10. This tell us that on average, we'll have 0.2 * 10, so 2 customers in the queue. This is our work in progress, often called WIP for short.
*** λ = L/W
We can rearrange this formula to look at different quantities. For example, our _throughput_ - the total number of customers we serve, or the tonnage of cement, or the number of changes we make to our software - is given by the WIP divided by the cycle time. This means that if we want to deliver more things, more tons of cement, more cups of coffee, more changes to our software, we should _reduce work in progress_ and make each item as small as possible. This is counter-intuitive, because it seems like if we work on more things in parallel, we'll get more done, but we'll actually deliver more value if we work on a smaller number of things.
*** W = L/λ
Not only that, but the _cycle_ time - the amount of time it takes us to deliver a single item - is governed by WIP. If we work on a smaller number of things, then each thing will take less time to go through the system, so we can be more reactive to changes.
** Aim for continuous flow
So the Toyota Production System aims to maximise flow through the system - the throughput, and it uses a bunch of simple principles.
*** Make the work visible
The first principle is straightforward - you can't control the flow of work if you don't know what work is _happening_. This is why I can be a stickler for getting work into Linear and making sure that things that are in progress are marked in-progress. It's not because I want to check up on what people are doing, it's because _seeing_ the work is the first step to being able to optimise it. If everyone is off doing random things in the background, then we have high work-in-progress, and that means less throughput and longer lead times.
*** Reduce waste
The second principle is that we should reduce waste. In the Toyota Production System, and Kanban, the agile framework it inspired, we talk about three types of waste.
**** Muda
Muda means "uselessness" or "futility" and covers "non-value adding activity". Some of this is _toil_ the stuff I spoke about at the beginning. Muda covers yak-shaving, and all the stupid stuff you have to do just to keep things ticking over. It also covers a bunch of less intuitive concepts
***** Over-production
Over-production is doing work that you can't use yet. Sometimes people finish up a task and decide to "get ahead" by starting something new. This increases work in progress. Instead, it's better to see if you can help with something that's already happening or - you know - just chill. It's okay to spend a few hours learning about cement, or practicing your cloud skills. You need to have _slack_ in the system for reasons we'll get to.
***** Over-processing
Over-processing is when you have to do too much work to complete a step in the process. In the coffee shop, if you clean the grinder after every use, or you boil the kettle repeatedly, that's over-processing. It's making work that doesn't have to be done to achieve the outcomes. If you're a software engineer and you're given a straightforward task to fetch data from a database for one customer, and you spend an extra three days to make it generic for any possible future customer - that's over-processing.
***** Defects
Sometimes things go wrong and you have to start over. Maybe you're making a pour-over in the coffee shop, but you accidentally grind for an espresso. If you do that, the water won't drain through the coffee and it'll come out bitter. The later in the process you discover the defect, the more expensive it is to fix. If you immediately notice that the coffee is too fine, you can throw it away and grind again. If you notice while you're brewing, you have to chuck away the coffee, re-grind, get another filter, boil the kettle. If you don't notice until you've delivered a bitter, astringent coffee to your customer, you have to start all over AND you've made the customer unhappy. In programming we rely on techniques like pair programming and test-driven design to identify and fix defects as early as possible.
***** Waiting
Waiting happens when work is handed over from one step to another, but the second step isn't ready to go. So long as the work isn't progressing, it just sits around getting cold, and accumulating cycle time. Common waits in software engineering include waiting for code reviews, or for testing to take place.
**** Mura
Mura is "unevenness". Imagine that in our coffee shop we have one person on the grinder, one person pouring drinks, and one person serving customers. If every customer orders a single coffee, then it's _easy_ for us to keep all three busy at all times. Grind, brew, serve, over and over. If someone comes in and wants to order 10 coffees, then we have unevenness in the system. We need to grind 10 lots of coffee, while the brewer stands around, then we have to brew 10 coffees. The quickest way to handle this situation is actually to start each coffee individualy - grind one dose, start the brew, grind a dose, start the brew until the brewer is fully busy, then wait for them to have capacity before grinding again.
In software engineering, we handle this by trying to break work down so that each work item is _roughly_ the same size. It's fine to have some variation, but too much will throw off our predictability and slow things down.
**** Muri
The last kind of waste is Muri - overburdening. If we give people too much work to do, we slow them down. When the ten coffee orders come in, maybe the server could stop being a server and help with the brewing. This increases our capacity at the bottleneck, and smooths the flow of work. When we have experts in particular things - react, or ml, or AWS - we increase the risk of overburdening employees, driving them to burn out, and creating bottlenecks that slow our delivery.
* DevOps: The Accelerate book
Okay, nearly done with the history lesson, I swear. The last major shift in agile development came with the DevOps movement. DevOps is a _culture_ where engineers are responsible for managing their code in production. In the bad old days, companies hired "ops" engineers who would maintain infrastructure and deploy code. We can see why this is bad. It's a phased silo - work gets completed and then handed over to someone else for the "deployment" phase, just like in Waterfall, and we incur risk. It's also a source of waiting - ops engineers get overburded and become a bottleneck to delivering value. Now there are a bunch of companies who, instead, hire "devops" engineers, thereby recreating the pattern in the most ironic way possible.

Anyway, I'm not here to rant.

In 2016, Dr Nicole Forsgren and Jez Humble published Accelerate. If you read one book about software engineering, I recommend this one. What they did was speak to thousands of organisations to understand what practices they used, and how that affected their software delivery. Then they went away and crunched the data, and wrote it all up in this book. They discovered that some organisations were vastly more effective than others
** The effects of elite performance
Organisations were asked for data on their profitability, productivity, and market share. Accelerate research shows that elite performers are _twice_ as likely to exceed their targets on these metrics. These aren't just software companies, they're retail, logistics, pharmaceuticals. Elite software delivery performance helped them to exceed their commercial objectives. In 2017, the authors furthered their research into non-commercial objectives and found that high performers were also twice as likely to meet or exceed their targets for customer satisfaction, operating efficiency, achieving mission goals, and quality of goods produced.
** The four key metrics
The authors found that elite performance was predicted most strongly by four metrics, universally known as the four key metrics, or the DORA metrics.

*** Lead Time
This is the time it takes for a developer's work to get into production. It's closely related to cycle time. Smaller units of work, better automation, and less waiting contribute to shortening this gap.

*** Deployment Frequency
This is the time between us pushing code into production. Elite teams deploy several times a day, whenever they feel like it. Low performers might only release code once every six months. As we know from the beginning, this increases risk.

*** Mean time to resolution
This is the time it takes us to fix a problem when it happens. Sometimes things go wrong, the system goes down, someone writes a terrible bug. Elite teams optimise for fixing those issues quickly, by investing in observability - so you can see what's happening - and reducing lead time so we can get a fix out quiclky.

*** Change failure rate
This is the number of changes that we had to rollback or fix. It's a tricky measure to define, but it's an indicator of the quality of work earlier in the process.
** What does elite look like?
Surprisingly, elite performers excel on _all_ metrics simultaneously. Releasing code more often doesn't lead to more errors, it reduces the error rate, and it makes us faster at recovering from issues. You can see from this table that low performers release less often, have more defects, and spend way longer resolving problems when they occur, beause these factors are interrelated.

The book describes the technical practices that strongly predict elite performance, these include TDD, continuous delivery, and strong test-data management.

The data are clear. If you want your organisation to meet and exceed its commercial and non-commercial goals, then you should continually optimise for the key metrics.

* What works?
Okay, that's the end of the history lesson. I want to talk quickly about the best teams I've seen and what made them work.
** Collaboration
Back in 2008 I was working at Huddle when they were less mature than CarbonRe. We got an agile coach in and spent a bunch of time talking about XP and scrum, and learning how to do them well. At the time we had a team of 5 engineers, and we had the best standups I've ever attended. They were run by one of our front-end engineers. Every day we asked "are we on track to deliver the things we committed to?" and if not we figured out how to adjust course. We went through the work in progress and asked "what can we do to progress the highest priority piece of work?". Nobody started new work if there was something they could do to smooth the way for work in progress. As a result, we were highly productive and responsive to change.
** Flow
The best team I worked on at Huddle was called Breaking Bad for reasons that are lost in the mists of time. On Breaking Bad, we started out using SCRUM - 2 weekly iterations with a release at the end of the sprint, estimation, all that stuff, but we grew out of it. We improved our tooling so that we could release whenever we wanted, and usually released something every single day. We had a prioritised backlog, and pulled work from it as required. We still set up fortnightly planning sessions to make sure the backlog was in a good state, but we were able to change our priorities on a daily basis without disruption, because the team was in a constant state of smooth flow.
** Focus on improvement
The best retrospectives I ever attended were on the platform team at MADE. The team had been underperforming for a while, and I used the retrospectives as a way to shake things up. We did a lot of work on psychological safety, but we also built a culture of improvement. At that time we had maybe 10 or 15 software systems in different states of maturity. We knew we wanted to standardise and improve practices but didn't have a clear way to achieve that. In the end, I came up with a simple checklist of 20 items for each system - does it have documentation, is it deployed with docker, does it emit business metrics for monitoring and so on. We gave every system a score out of 20 for compliance and took an average. In each retrospective, we asked "what one thing could we do this fortnight to increase this number?". This paid off hugely, and within a few months we had fixed most of the issues in our systems, by regularly inspecting our current state and proposing cheap experiments to improve a metric.
** Learning culture
The most fun I've had with a team was the Last Mile team at Cazoo. This was a temporary team that was set up to tackle a major architectural refactoring. We had a big system, and we wanted to divide it up into smaller systems that would be easier to work with. The team had been stuck for months trying to figure out how to move forward. We started working as a mob: every day we sat together on Discord, with one person typing and the others talking and observing. Every 20 minutes we rotated, so the next person would take the hot seat. Progress was slow, but the team produced high quality code because all the engineers could have input on the right way to move forward. Moreover, this was the most effective learning environment I've ever seen. Participants were continually sharing ideas: neat tricks for the command line, better ways to write Typescript, weird quirks of AWS. There wasn't a single day that I didn't learn something new from the group. Working together so closely helped the team to gel, even though we were fully remote in the pandemic. The team had _difficult_ conversations about how we communicated as a group that would be challenging even in person.
* What should CRe do?
** Ban the word "deadline"
I said at the outset that software projects rarely have deadlines, so let's not use that word any more. We shouldn't replace it with the word "target" either. The problem is that the amount of time a task takes in software engineering is not deterministic, it's stochastic. Trying to impose a date on software engineering is like imposing a target for rain or for radioactive decay. It will snow on December the 25th or else.
** Prioritise predictability
But we can't ignore the value of planning and predictability. We can square this circle by providing _forecasts_ instead of targets. We can take the number of items a team completes every week, and the number of items remaining to deliver, and we can predict a delivery date. Using Monte-Carlo simulations for this gives us a probability distribution. We can say, for example, that there is a 95% probability we can deliver Voto Cuiaba by date X, but only a 65% chance we can deliver by date Y. Fortunately, Linear does this for us automatically, we just have to feed in the data.

For monte-carlo to work well, we need to be predictable as a team. That means that work has to be made visible - all work items get added to the board - and that each piece of work should be roughly the same size: no tickets callde "investigate X" that stay on the board for weeks without moving. These outliers make it harder to predict our performance, and harder for us to give forecasts to our customers and internal stakeholders.

For the team to be predictable, we need to have some slack in the system. We can't overload engineers, or they'll choke up blocking the flow, and we need to _help each other_ to deliver work more smoothly into production. Lower work-in-progress, and lower-cycle team equals higher throughput in _any_ system with queueing dynamics.
** Set clear focused goals for each cycle
For us to reduce work in progress and measure our delivery, it's useful to have a clear goal for each cycle. We should be able to gather at standup each day and ask "are we on track to meet our next goal?". Goals might be customer focused, eg. provide basic recommendations for Votorantim, or they might be technical - provide access to plant data through pandas, but we should collectively be focused on reaching a single goal at a time, rather than spreading ourselves thinly. Lower work in progress equals higher throughput.
** Track and continuously improve DORA metrics
Finally, we should recognise that software delivery performance correlates with organisational performance. Techniques like continuous delivery, test-driven development, and modern observability don't just make us better at building software, they make us better at selling our product, they make us better at serving our customers, and they make us better - ultimately - at abating carbon emissions in the gigaton range.
